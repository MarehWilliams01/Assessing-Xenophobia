{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1KA_LsObLAGRR9R2izz8xaOskHZFqOSQE",
      "authorship_tag": "ABX9TyNt5X6rYHsCwUF9uXMeZMos",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MarehWilliams01/Assessing-Xenophobia/blob/main/assessing_xenophobia.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q5yaNmPeLP2H"
      },
      "outputs": [],
      "source": [
        "# importing the neccessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import spacy\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from textblob import TextBlob\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# setting up the cleaned xenophobia dataset\n",
        "\n",
        "data = pd.read_csv('/content/drive/MyDrive/Datasets/SA_XenoDataset_2017-2022.csv')\n",
        "df = pd.DataFrame(data.drop(columns=[ 'Unnamed: 0','Likes', 'No of replies', 'Language', 'Coordinates', 'Number of Attacks','Sentiment', 'User Location' ,'District', 'Province'], axis=1)) #converting it to dataframe\n",
        "\n",
        "# converting 'TimeCreated' and 'DateCreated' to be of the type datetime\n",
        "# for 'TimeCreated' column\n",
        "df['TimeCreated'] = pd.to_datetime(df['TimeCreated'])\n",
        "df['TimeCreated'] = pd.to_datetime(df['TimeCreated'], format='%H:%M:%S').dt.time\n",
        "\n",
        "# for 'DateCreated' column\n",
        "df['DateCreated'] = pd.to_datetime(df['DateCreated'])\n",
        "df['DateCreated'] = df['DateCreated'].dt.date\n",
        "\n",
        "# combine 'DateCreated' and 'TimeCreated' columns into a single datetime column\n",
        "df['DateTimeCreated'] = pd.to_datetime(df['DateCreated'].astype(str) + ' ' + df['TimeCreated'].astype(str))\n",
        "\n",
        "# xxtract year from 'DateTimeCreated' and create a new column\n",
        "df['Year'] = df['DateTimeCreated'].dt.year\n",
        "\n",
        "# dropping rows with NaN Inputs\n",
        "df= df.dropna()\n",
        "\n",
        "df"
      ],
      "metadata": {
        "id": "krEm_UxTMZfq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocessing tweets\n",
        "df['CleanedTweet'].apply(type)"
      ],
      "metadata": {
        "id": "UqKeIypj1h0z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# developing a dictionary for shorthand texts\n",
        "# send a GET request\n",
        "url = \"https://messente.com/blog/text-abbreviations\"\n",
        "response = requests.get(url)\n",
        "\n",
        "# parse the HTML content using BeautifulSoup\n",
        "soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "# find the second and third b tags and extract the data\n",
        "b_tags = soup.find_all(\"b\")\n",
        "second_b_tag = b_tags[1]\n",
        "third_b_tag = b_tags[2]\n",
        "\n",
        "# find all paragraphs inside the second and third b tags, excluding the first paragraph\n",
        "second_paragraphs = second_b_tag(\"p\")[1:]\n",
        "third_paragraphs = third_b_tag(\"p\")[1:]\n",
        "\n",
        "# extract the slang words and descriptions\n",
        "slang_list = []\n",
        "description_list = []\n",
        "count = 0\n",
        "\n",
        "for paragraph in second_paragraphs + third_paragraphs:\n",
        "  if count <= 99:\n",
        "    split_text = paragraph.text.strip().split(\" â€“ \", 1)\n",
        "    slang = split_text[0].split(\". \", 1)[-1].lower()\n",
        "    description = split_text[1] if len(split_text) > 1 else \"\"\n",
        "    slang_list.append(slang)\n",
        "    description_list.append(description)\n",
        "    count += 1\n",
        "\n",
        "  else:\n",
        "    break\n",
        "\n",
        "# create a DataFrame from the extracted data\n",
        "df_slangs = pd.DataFrame({\n",
        "    \"slang\": slang_list,\n",
        "    \"description\": description_list\n",
        "})\n",
        "\n",
        "# print the DataFrame\n",
        "print(df_slangs)"
      ],
      "metadata": {
        "id": "NEtwqeon2u-p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocessing the data\n",
        "\n",
        "# developing a function to clean the data\n",
        "def clean_tweet(text):\n",
        "  words = text.split() # splits text into each word\n",
        "  normalized_words = [df_slangs.loc[df_slangs['slang'] == word, \"description\"].values[0]\n",
        "    if word in df_slangs['slang'].values else word for word in words] # removes slangs\n",
        "  text = \" \".join(normalized_words)\n",
        "\n",
        "  return text.lower()\n",
        "\n",
        "# checking for NaN values in the 'CleanedTweet' column\n",
        "df['CleanedTweet'] = df['CleanedTweet'].apply(clean_tweet)\n",
        "\n",
        "df['CleanedTweet']"
      ],
      "metadata": {
        "id": "-pWsn6w_22qv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lemmatization\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# function to lemmatize each word in a tweet\n",
        "def lemmatize_text(text):\n",
        "    doc = nlp(text)\n",
        "    lemmas = [token.lemma_ for token in doc]\n",
        "    return ' '.join(lemmas)\n",
        "\n",
        "# apply lemmatization to the 'CleanedTweet' column\n",
        "df['CleanedTweet'] = df['CleanedTweet'].apply(lemmatize_text)\n",
        "\n",
        "df['CleanedTweet']"
      ],
      "metadata": {
        "id": "_WgK1jHm2_p0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# removing stopwords\n",
        "\n",
        "# getting the current list of English stopwords\n",
        "stopword_list = stopwords.words('english')\n",
        "\n",
        "# function to remove stopwords from tweet\n",
        "def remove_stopwords(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
        "    text = ' '.join(filtered_tokens)\n",
        "    return text\n",
        "\n",
        "# apply stopword removal to the tweet column\n",
        "df['CleanedTweet'] = df['CleanedTweet'].apply(remove_stopwords)\n",
        "\n",
        "df['CleanedTweet']"
      ],
      "metadata": {
        "id": "sNa_gsHK3SVE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to get sentiment using TextBlob\n",
        "def get_sentiment(text):\n",
        "    analysis = TextBlob(text)\n",
        "    if analysis.sentiment.polarity > 0:\n",
        "        return 'Positive'\n",
        "    elif analysis.sentiment.polarity < 0:\n",
        "        return 'Negative'\n",
        "    else:\n",
        "        return 'Neutral'\n",
        "\n",
        "# apply the sentiment analysis function to the tweet column\n",
        "df['Sentiment'] = df['CleanedTweet'].apply(get_sentiment)\n",
        "\n",
        "df"
      ],
      "metadata": {
        "id": "D7tT4tNge3Xe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# analyzing sentiment on xenophobia tweets over time\n",
        "\n",
        "# seeing the different dates used in the dataset\n",
        "# get unique time values\n",
        "unique_times = df['DateCreated'].unique()\n",
        "\n",
        "# display unique time values\n",
        "for time_val in unique_times:\n",
        "    print(time_val)\n",
        "\n",
        "\n",
        "# grouping the data by year and sentiment, and count the sentiments\n",
        "sentiment_counts = df.groupby(['Year', 'Sentiment']).size().unstack(fill_value=0)\n",
        "\n",
        "# display the result\n",
        "print(sentiment_counts)\n"
      ],
      "metadata": {
        "id": "Yu7oQ8J3bXf_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# finding the average sentiment of xenophobia-related tweets over time in years\n",
        "\n",
        "# function to map sentiment labels to numerical values\n",
        "def map_sentiment_to_numeric(sentiment):\n",
        "    if sentiment == 'Negative':\n",
        "        return -1\n",
        "    elif sentiment == 'Positive':\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "# apply the mapping function to the 'Sentiment' column and creating a new column\n",
        "df['SentimentNumeric'] = df['Sentiment'].apply(map_sentiment_to_numeric)\n",
        "\n",
        "# extract year from 'DateTimeCreated' and creating a new column\n",
        "df['Year'] = df['DateTimeCreated'].dt.year\n",
        "\n",
        "# grouping the data by year and sentiment, and calculating the most common sentiment\n",
        "sentiment_counts = df.groupby(['Year', 'SentimentNumeric']).size().unstack(fill_value=0)\n",
        "most_common_sentiment = sentiment_counts.idxmax(axis=1)\n",
        "\n",
        "# creating the line chart\n",
        "plt.figure(figsize=(10, 6))\n",
        "most_common_sentiment.plot(kind='line', marker='o')\n",
        "plt.title('Average Sentiment of Xenophobia-Related Tweets Over Time')\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Average Sentiment')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Qob0IlDtFTsu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# assessing long-term trends\n",
        "\n",
        "# calculate sentiment proportions for each year\n",
        "sentiment_proportions = df.groupby(['Year', 'Sentiment']).size().unstack(fill_value=0).div(df.groupby('Year').size(), axis=0)\n",
        "\n",
        "# for neutral sentiments\n",
        "# reshape the data for linear regression\n",
        "X = sentiment_proportions.index.values.reshape(-1, 1)\n",
        "y = sentiment_proportions['Neutral']  # Replace 'Negative' with 'Positive' or 'Neutral' as needed\n",
        "\n",
        "# fit linear regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "# predict sentiment proportions using the model\n",
        "predicted_sentiment_proportions = model.predict(X)\n",
        "\n",
        "# plot the results\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(sentiment_proportions.index, sentiment_proportions['Neutral'], color='blue', label='Actual')\n",
        "plt.plot(sentiment_proportions.index, predicted_sentiment_proportions, color='red', label='Trend Line')\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Proportion of Neutral Sentiments')\n",
        "plt.title('Long-Term Sentiment Trends')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# for negative sentiments\n",
        "# reshape the data for linear regression\n",
        "X = sentiment_proportions.index.values.reshape(-1, 1)\n",
        "y = sentiment_proportions['Negative']  # Replace 'Negative' with 'Positive' or 'Neutral' as needed\n",
        "\n",
        "# fit linear regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "# predict sentiment proportions using the model\n",
        "predicted_sentiment_proportions = model.predict(X)\n",
        "\n",
        "# plot the results\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(sentiment_proportions.index, sentiment_proportions['Negative'], color='blue', label='Actual')\n",
        "plt.plot(sentiment_proportions.index, predicted_sentiment_proportions, color='red', label='Trend Line')\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Proportion of Negative Sentiments')\n",
        "plt.title('Long-Term Sentiment Trends')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# for positive sentiments\n",
        "# reshape the data for linear regression\n",
        "X = sentiment_proportions.index.values.reshape(-1, 1)\n",
        "y = sentiment_proportions['Positive']  # Replace 'Negative' with 'Positive' or 'Neutral' as needed\n",
        "\n",
        "# fit linear regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "# predict sentiment proportions using the model\n",
        "predicted_sentiment_proportions = model.predict(X)\n",
        "\n",
        "# plot the results\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(sentiment_proportions.index, sentiment_proportions['Positive'], color='blue', label='Actual')\n",
        "plt.plot(sentiment_proportions.index, predicted_sentiment_proportions, color='red', label='Trend Line')\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Proportion of Positive Sentiments')\n",
        "plt.title('Long-Term Sentiment Trends')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "-HGCPxXqs4Xi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extract month\n",
        "df['Month'] = df['DateTimeCreated'].dt.month\n",
        "\n",
        "# set a threshold for detecting significant changes\n",
        "threshold = 0.05\n",
        "\n",
        "# creating a subplot for each year\n",
        "years = df['Year'].unique()\n",
        "num_years = len(years)\n",
        "fig, axes = plt.subplots(nrows=num_years, ncols=1, figsize=(10, 6 * num_years))\n",
        "\n",
        "# looping through each year\n",
        "for i, year in enumerate(years):\n",
        "    # filter data for the current year\n",
        "    year_df = df[df['Year'] == year]\n",
        "\n",
        "    # calculating sentiment distribution for each month\n",
        "    sentiment_distribution = year_df.groupby('Month')['SentimentNumeric'].mean()\n",
        "    sentiment_distribution_percentage = sentiment_distribution / sentiment_distribution.sum()\n",
        "\n",
        "    # calculating overall sentiment distribution\n",
        "    overall_sentiment_distribution = sentiment_distribution.sum() / sentiment_distribution.sum()\n",
        "\n",
        "    # identify months with notable changes in sentiment distribution\n",
        "    notable_months = sentiment_distribution_percentage[\n",
        "        (sentiment_distribution_percentage - overall_sentiment_distribution).abs() > threshold\n",
        "    ]\n",
        "\n",
        "    # plot the sentiment distribution and mark notable months\n",
        "    ax = axes[i]\n",
        "    ax.scatter(notable_months.index, notable_months, color='red', label='Notable Months', s=50)\n",
        "    ax.set_xlabel('Month')\n",
        "    ax.set_ylabel('Sentiment Distribution')\n",
        "    ax.set_title(f'Notable Sentiment Distribution for Year {year}')\n",
        "    ax.set_xticks(range(1, 13))\n",
        "    ax.set_xticklabels(['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'], rotation=0)\n",
        "    ax.legend()\n",
        "    ax.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "VQA-dZ4GGO7G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# group the data by year and sentiment, and calculate the most common sentiment\n",
        "sentiment_counts = df.groupby(['Tweet Origin', 'Sentiment']).size().unstack(fill_value=0)\n",
        "most_common_sentiment = sentiment_counts.idxmax(axis=1)\n",
        "\n",
        "# create a bar plot to visualize sentiment variations across regions\n",
        "plt.figure(figsize=(50, 20))\n",
        "plt.scatter(most_common_sentiment.index, most_common_sentiment.values, color='skyblue')\n",
        "plt.xlabel('Region', fontsize=14)\n",
        "plt.ylabel('Average Sentiment Score', fontsize=14)\n",
        "plt.title('Sentiment Variations Across Different Regions', fontsize=16)\n",
        "plt.xticks(rotation=45, fontsize=17)\n",
        "plt.yticks(fontsize=17)\n",
        "plt.tight_layout()\n",
        "\n",
        "# show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "LMifFCahNRLT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# defining high and low sentiment thresholds\n",
        "high_sentiment_threshold = 0.5\n",
        "low_sentiment_threshold = -0.5\n",
        "\n",
        "# segment data into high and low sentiment periods\n",
        "high_sentiment_periods = df[df['SentimentNumeric'] > high_sentiment_threshold]\n",
        "low_sentiment_periods = df[df['SentimentNumeric'] < low_sentiment_threshold]\n",
        "\n",
        "# function to calculate user engagement metrics\n",
        "def calculate_engagement_metrics(data):\n",
        "    avg_sentiment = data['SentimentNumeric'].mean()\n",
        "    retweet_count = data['Retweets'].mean()\n",
        "    positive_message_share = data[data['Sentiment'] == 'Positive'].shape[0] / data.shape[0]\n",
        "    return avg_sentiment, retweet_count, positive_message_share\n",
        "\n",
        "# calculate engagement metrics for high sentiment periods\n",
        "high_avg_sentiment, high_retweet_count, high_positive_share = calculate_engagement_metrics(high_sentiment_periods)\n",
        "\n",
        "# calculate engagement metrics for low sentiment periods\n",
        "low_avg_sentiment, low_retweet_count, low_positive_share = calculate_engagement_metrics(low_sentiment_periods)\n",
        "\n",
        "# create a bar chart for visualization\n",
        "metrics = ['Average Sentiment', 'Average Retweets', 'Positive Message Share']\n",
        "high_values = [high_avg_sentiment, high_retweet_count, high_positive_share]\n",
        "low_values = [low_avg_sentiment, low_retweet_count, low_positive_share]\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(metrics, high_values, color='blue', label='High Sentiment Periods')\n",
        "plt.bar(metrics, low_values, color='red', label='Low Sentiment Periods', alpha=0.5)\n",
        "plt.ylabel('Metrics')\n",
        "plt.title('User Engagement Patterns During High and Low Sentiment Periods')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "yB9A26rrKxo7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}